{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On dynamic range: 'roundoff errors' and the FFT\n",
    "\n",
    "### Background\n",
    "Accuracy of the FFT has been a consideration since (at least) the initial implementation of the Cooley-Tukey algorithm, there are a handful papers on the subject going right back to the 60's - 'accuracy' and 'round-off error' being the most relevant search terms. The best recent references I could find are:\n",
    "\n",
    "* [Accuracy of the Discrete Fourier Transform and the Fast Fourier Transform (Schatzman 1996)](https://doi.org/10.1137/S1064827593247023) - a relatively recent review of the theory\n",
    "\n",
    "* [Implementing FFTs in Practice (Johnson et al. 2008)](http://cnx.org/content/m16336/) - from the authors of FFTW\n",
    "\n",
    "The consensus seems to be that the relative error from a (carefully implemented and tuned) FFT of input length N grows as $O(\\log{N})$ in the worst case, and $O(\\sqrt{\\log{N}})$ on average for the more amenable case of random input data. This applies to one-dimensional FFT's, but (perhaps surprisingly) the error-growth curve for a 2-dimensional FFT is only slightly worse and of the same order (cf $\\S6$, Schatzman 1996).\n",
    "\n",
    "\n",
    "However, it's worth noting that the typical results (as reported in the [FFTW accuracy benchmarks](http://www.fftw.org/accuracy/comments.html)) are obtained by testing with random data (computed with arbitrarily high precision and then compare with results computed via standard single / double floating precision). So when considering a radio-field with a bright source and a faint source, we may be in the 'worst-case' regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a typical benchmark plot for FFTW:\n",
    "![Illustrative FFTW benchmark plot](http://www.fftw.org/accuracy/Ryzen-7-3.6GHz/ryzen.1d.scxx.acc.p2.png)\n",
    "\n",
    "This has a similar shape to the best-case curves from Schatzman 1996 (cf $\\S3.4$).\n",
    "For an FFT of length $N$ we expect a relative error in the random-noise 'typical' cose of order $$\\sqrt{\\log{N}}\\epsilon = 3\\epsilon$$\n",
    "where $\\epsilon$ is the machine accuracy --- recall this is  $\\frac{1}{2^{24}}$ in the single-precision case.\n",
    "So we can run a quick calculation, generate our own plot, and check if the FFTW plot agrees with our understanding of the theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "import numpy as np\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "\n",
    "N = np.asarray([float(2**i) for i in range(4,18)])\n",
    "epsilon = 1./ 2**24\n",
    "arbitrary_scaling_factor = 0.6 # cf Schatzman 1996, error function 3b\n",
    "\n",
    "def fft_err(N):\n",
    "    return arbitrary_scaling_factor* np.sqrt(np.log2(N))  * epsilon\n",
    "\n",
    "plt.plot(N, fft_err(N))\n",
    "ax = plt.gca()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log', basex=2)\n",
    "\n",
    "# plt.xticks(N)\n",
    "plt.xlim(min(N), max(N))\n",
    "plt.ylim(0.5e-7, 1.5e-7)\n",
    "plt.axhline(1e-7, ls=':')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Relative error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result - not a perfect agreement, but in the right ballpark."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
